
## Kafka Extended API
- Kafka Connect
- Kafka Streams
- Kafka Schema Registry


### Kafka Connect
- 외부 Data Source들과 연결하기 위해 만든 Kafka Connector(Ex. Jdbc 등과 비슷한 개념인듯?)
- Kafka와 외부 DataSource들과 데이터를 잘 전달하기 위해서 만든 라이브러리
- ![image](https://user-images.githubusercontent.com/15210906/119509589-bcd40180-bdab-11eb-9a8c-2cd26ef1a651.png)



### Kafka Streams
- Kafka에서 받은 데이터를 Processing 하거나 Transform하는 데에 사용되는 라이브러리
- 생산성을 높이는 라이브러리(직접 구현하는 것은 너무 low)
- ![image](https://user-images.githubusercontent.com/15210906/119507890-34089600-bdaa-11eb-9311-ee37ae816d1c.png)


### Kafka Schema Registry
- 카프카는 bytes로 input을 받고 bytes로 publish함(바이너리 데이터만 왔다갔다함)
- 그래서 카프카는 data에 대한 검증을 하지 않는다(못한다) -> data format에 대한 정보가 없음
- 그래서 데이터 포맷에 대한 정의(Schema)를 우리가 해야 하고, 이를 등록하는 곳이 Schema Registry 이다.
- Schema Registry는 Consumer에서 올바른 데이터를 받도록 bad data를 걸러내기 위해서 사용한다~
- As-Is
![image](https://user-images.githubusercontent.com/15210906/119505861-57324600-bda8-11eb-85e7-e1a0069dc4bc.png)

- To-Be
![image](https://user-images.githubusercontent.com/15210906/119505885-5d282700-bda8-11eb-9cce-3df87393f4f5.png)

- 단점
  - 설치하는데 오래 걸린다.
  - Producer, Consumer 코드를 일부 수정해야함.
  - 러닝커브가 있다.

- Avro schema Example
![image](https://user-images.githubusercontent.com/15210906/119506784-35858e80-bda9-11eb-8198-50016d954236.png)

